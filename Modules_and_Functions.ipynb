{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Namee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from twitterscraper import query_tweets\n",
    "import datetime as dt\n",
    "#reading excel file and importing\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "import builtins\n",
    "import pickle\n",
    "import urllib.request                   \n",
    "from bs4 import BeautifulSoup           \n",
    "from time import sleep\n",
    "from random import uniform\n",
    "from selenium import webdriver\n",
    "import selenium\n",
    "import time\n",
    "from matplotlib.pyplot import subplots\n",
    "from IPython.display import Image, display\n",
    "tokenizer = TweetTokenizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"&amp;\", \"&gt;\", \"&lt;\"])\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading pickeled news handles for fact based classification\n",
    "#import pickle\n",
    "with open(\"final_handle_list.pkl\", \"rb\") as f:\n",
    "    final_handle_list = pickle.load(f) \n",
    "#Reading pickeled word repositiories representing Databreach and Racial categories\n",
    "with open(\"word_repositories.pkl\", \"rb\") as f:\n",
    "    word_repositories = pickle.load(f) \n",
    "#Reading pickled top hashtag list for each category\n",
    "with open(\"classification_hashtags.pkl\", \"rb\") as f:\n",
    "    classification_hashtags = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#To fetch data from twitter using the handles and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To fetch data from twitter using the handles and dates\n",
    "def fetchData(issue,about,begin_date,end_date):\n",
    "    issue=query_tweets(about,begindate=begin_date,enddate=end_date,limit=20000,lang='english')\n",
    "    return issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to use when query is available directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to use when query is available directly\n",
    "def fetchQueryData(issue,query):\n",
    "    issue=query_tweets(query)\n",
    "    return issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#To clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To clean the data\n",
    "def clean_tweets(tokenized_tweets):\n",
    "    clean_text = []\n",
    "    handles = []\n",
    "    hashtags = []\n",
    "    tweet_len = []\n",
    "    for tweet in tokenized_tweets:\n",
    "        thandles = []\n",
    "        thashtags = []\n",
    "        joined_words = \" \"\n",
    "        words = [w.lower() for w in tweet if len(w)>2 and w not in stop_words]\n",
    "        thandles = [w for w in words if re.search(\"^@\\w+\", w)]\n",
    "        thashtags = [w for w in words if re.search(\"^#\\w+\", w)]\n",
    "        words = [w for w in words if w not in thandles and w not in thashtags]\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "        joined_words = joined_words.join(words)\n",
    "        tlength = len(joined_words)\n",
    "        handles.append(thandles)\n",
    "        hashtags.append(thashtags)\n",
    "        clean_text.append(joined_words)\n",
    "        tweet_len.append(tlength)\n",
    "    return clean_text,tweet_len,handles,hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to get the urls from string format of url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to get the urls from string format of url\n",
    "def get_url(urllist):\n",
    "    get_url=[]\n",
    "    for url in urllist:\n",
    "        try:\n",
    "            res = urlopen(url)\n",
    "            actual_url = res.geturl()\n",
    "            get_url.append(actual_url)\n",
    "        except:\n",
    "            get_url.append(url)\n",
    "    return get_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to get the url list to webscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to get the url list to webscrape     \n",
    "def get_urllist(dataframe):\n",
    "    valid_url_list=[]\n",
    "    for tweet in tqdm(dataframe[\"text\"]):\n",
    "        urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet)\n",
    "        for url in urls:\n",
    "            for handle in final_handle_list:\n",
    "                if(len(handle)!=0):\n",
    "                    if handle.lower() in url.lower():\n",
    "                        valid_url_list.append(url)\n",
    "    return valid_url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to get hashtag lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get hashtag lists\n",
    "def classify_hashtag(dataframe):\n",
    "    klist=[]\n",
    "    hashtag_list=[]\n",
    "    list_of_hashtags=[]\n",
    "    for i in dataframe['hashtags']:\n",
    "            if len(i)>0:\n",
    "                for element in i:\n",
    "                    if element not in klist:\n",
    "                        klist.append(element)\n",
    "    hashtag_list.append(klist)       \n",
    "    flat_list = [item for sublist in hashtag_list for item in sublist]\n",
    "    dict = {} \n",
    "    for w in flat_list:\n",
    "        dict[w] = dict.get(w, 0) + 1\n",
    "\n",
    "    word_freq = []\n",
    "    for key, value in dict.items():\n",
    "        word_freq.append((value, key))\n",
    "    word_freq.sort(reverse=True)\n",
    "    values, labels = zip(*word_freq)\n",
    "    list_of_hashtags=list(set(labels))\n",
    "    return list_of_hashtags\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Function to check the category through webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def webscrape_count(url_list):\n",
    "    #Function to scrape th url\n",
    "    Racial_count=0\n",
    "    Databreachcount=0\n",
    "    Other_count=0\n",
    "    for url in url_list:\n",
    "        driver = webdriver.Chrome(\"C://chromedriver_win32/chromedriver.exe\")\n",
    "        driver.implicitly_wait(25)\n",
    "        try:     \n",
    "            driver.get(url)\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            text_string=\" \"\n",
    "            for i in soup.find_all('p'):\n",
    "                text_string=text_string+(i.text)\n",
    "            for word in word_repositories[\"Databreach\"]:\n",
    "                if(word in text_string):\n",
    "                    Databreachcount=Databreachcount+1\n",
    "            for word in word_repositories[\"Racial\"]:\n",
    "                if(word in text_string):\n",
    "                    Racial_count=Racial_count+1                   \n",
    "        except:\n",
    "            continue\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    print(\"Data Breach count:\",Databreachcount,Racial_count)\n",
    "    print(\"Racial count:\",Racial_count)\n",
    "    return Databreachcount,Racial_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
